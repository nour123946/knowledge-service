# app/llm/groq_llm.py
import os
from groq import Groq
from app.core.memory import get_history
import logging

logger = logging.getLogger(__name__)

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

def generate_response(question, context_chunks, session_id="default"):
    """
    Generate SHORT and DIRECT response using Groq LLM
    
    CB-4: LLM Response Generation
    CB-7: Conversation Memory
    """

    # ğŸ”¹ Flatten chunks
    flat_chunks = []
    for item in context_chunks:
        if isinstance(item, list):
            flat_chunks.extend(item)
        else:
            flat_chunks.append(item)

    context_text = "\n".join(flat_chunks) if flat_chunks else "Pas d'info disponible."

    # ğŸ”¹ Memory
    history = get_history(session_id, last_n=6)
    history_text = ""
    for msg in history:
        role = "Client" if msg['role'] == "user" else "Bot"
        history_text += f"{role}: {msg['content']}\n"

    # ğŸ”¥ PROMPT COURT ET DIRECT
    prompt = f"""Tu es un assistant commercial EFFICACE et CONCIS.

ğŸ“Œ **RÃˆGLE ABSOLUE : RÃ‰PONDS EN 2-3 PHRASES MAX !**

ğŸ“‹ **BASE DE DONNÃ‰ES :**
{context_text}

âœ… **EXEMPLES DE BONNES RÃ‰PONSES :**

Client: "merci"
Bot: "De rien ! ğŸ˜Š"

Client: "bonjour"
Bot: "Bonjour ! Comment puis-je vous aider ?"

Client: "je veux adidas"
Bot: "Parfait ! Les Adidas Ultraboost sont Ã  420 TND. Vous voulez les commander ?"

Client: "avez vous iphone"
Bot: "DÃ©solÃ©, on n'a pas d'iPhone. On a des chaussures : Puma (310 TND), Adidas (420 TND), Converse (190 TND). Ã‡a vous intÃ©resse ?"

Client: "combien coÃ»te les puma"
Bot: "Les Puma RS-X coÃ»tent 310 TND. ğŸ˜Š"

Client: "je veux passer commande"
Bot: "Super ! Quel produit vous intÃ©resse ?"

âŒ **INTERDIT :**
- Ã‰crire plus de 3 phrases
- Raconter des dÃ©tails inutiles
- RÃ©pÃ©ter les infos
- Parler de politique de retour sauf si demandÃ©

---

ğŸ“œ **HISTORIQUE :**
{history_text}

â“ **CLIENT :**
{question}

ğŸ’¬ **TA RÃ‰PONSE (COURTE ET DIRECTE) :**"""

    try:
        chat_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.3-70b-versatile",
            temperature=0.5,  # ğŸ”¥ Moins crÃ©atif = plus concis
            max_tokens=150    # ğŸ”¥ LIMITÃ‰ Ã€ 150 tokens
        )

        response = chat_completion.choices[0].message.content.strip()
        logger.info(f"âœ… Short response generated for session {session_id}")
        return response
    
    except Exception as e:
        logger.error(f"âŒ Groq API error: {e}")
        return "DÃ©solÃ©, problÃ¨me technique. Un conseiller va vous aider. ğŸ˜Š"